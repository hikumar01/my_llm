# Environment Configuration for C++ AI Assistant with Local LLM Models

# ============================================================================
# STORAGE PATHS
# ============================================================================

# Host repositories directory (absolute path - customize this!)
# This is where your C++ source code repositories are located
HOST_REPOS_DIR=~/src

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Comma-separated list of models to use (by key)
# Options: deepseek-coder,codellama,qwen2.5-coder,starcoder2
# Use "all" to enable all models
ENABLED_MODELS=all

# Auto-pull models on server startup if not cached (default: true)
# Set to false to disable automatic model downloads
AUTO_PULL_MODELS=true

# ============================================================================
# PERFORMANCE TUNING (OPTIONAL)
# ============================================================================

# Number of parallel workers for symbol extraction (default: 4)
# SYMBOL_EXTRACTOR_WORKERS=4

# Batch size for database inserts (default: 100)
# SYMBOL_BATCH_SIZE=100

# Batch size for embedding generation (default: 32)
# EMBEDDING_BATCH_SIZE=32

# File watcher debounce time in seconds (default: 2.0)
# WATCH_DEBOUNCE_SECONDS=2.0

# ============================================================================
# DEVELOPMENT CONFIGURATION (OPTIONAL)
# ============================================================================

# Enable development mode (mounts source code for live updates)
DEV_MODE=true

